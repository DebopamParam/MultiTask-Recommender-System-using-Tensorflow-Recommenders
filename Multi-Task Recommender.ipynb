{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwrcZeK7x7xI"
      },
      "source": [
        "## Imports\n",
        "\n",
        "\n",
        "Let's first get our imports out of the way.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vV6cAKxOYRp-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izzoRqkGb2Zc"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZGYDaF-m5wZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxQ_hy7xPH3N"
      },
      "outputs": [],
      "source": [
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PAqjR4a1RR4"
      },
      "source": [
        "## Preparing the dataset\n",
        "\n",
        "We're going to use the Movielens 100K dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ySWtibjm_6a"
      },
      "outputs": [],
      "source": [
        "ratings = tfds.load('movielens/latest-small-ratings', split=\"train\")\n",
        "movies = tfds.load('movielens/latest-small-movies', split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OnvW8Sh32jp"
      },
      "outputs": [],
      "source": [
        "for x in movies.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)\n",
        "print(\"------------------------\")\n",
        "for x in ratings.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhBSlejf3znS"
      },
      "outputs": [],
      "source": [
        "# Select the basic features.\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"user_rating\": x[\"user_rating\"],\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRHorm8W1yf3"
      },
      "source": [
        "And repeat our preparations for building vocabularies and splitting the data into a train and a test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS0eDfkjnjJL"
      },
      "outputs": [],
      "source": [
        "# Randomly shuffle data and split between train and test.\n",
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)\n",
        "\n",
        "movie_titles = movies.batch(1_000)\n",
        "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
        "\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCi-seR86qqa"
      },
      "source": [
        "## A multi-task model\n",
        "\n",
        "There are two critical parts to multi-task recommenders:\n",
        "\n",
        "1. They optimize for two or more objectives, and so have two or more losses.\n",
        "2. They share variables between the tasks, allowing for transfer learning.\n",
        "\n",
        "In this tutorial, we will define our models as before, but instead of having  a single task, we will have two tasks: one that predicts ratings, and one that predicts movie watches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXHrk_SLzKCM"
      },
      "source": [
        "The user and movie models are as before:\n",
        "\n",
        "```python\n",
        "user_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_user_ids, mask_token=None),\n",
        "  # We add 1 to account for the unknown token.\n",
        "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "])\n",
        "\n",
        "movie_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_movie_titles, mask_token=None),\n",
        "  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
        "])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWCwkE5z8QBe"
      },
      "source": [
        "However, now we will have two tasks. The first is the rating task:\n",
        "\n",
        "```python\n",
        "tfrs.tasks.Ranking(\n",
        "    loss=tf.keras.losses.MeanSquaredError(),\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrgQIXEm8UWf"
      },
      "source": [
        "Its goal is to predict the ratings as accurately as possible.\n",
        "\n",
        "The second is the retrieval task:\n",
        "\n",
        "```python\n",
        "tfrs.tasks.Retrieval(\n",
        "    metrics=tfrs.metrics.FactorizedTopK(\n",
        "        candidates=movies.batch(128)\n",
        "    )\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCNrv7_gakmF"
      },
      "source": [
        "As before, this task's goal is to predict which movies the user will or will not watch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSWw3xuq8mGh"
      },
      "source": [
        "### Putting it together\n",
        "\n",
        "We put it all together in a model class.\n",
        "\n",
        "The new component here is that - since we have two tasks and two losses - we need to decide on how important each loss is. We can do this by giving each of the losses a weight, and treating these weights as hyperparameters. If we assign a large loss weight to the rating task, our model is going to focus on predicting ratings (but still use some information from the retrieval task); if we assign a large loss weight to the retrieval task, it will focus on retrieval instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFSkOAMgzU0K"
      },
      "outputs": [],
      "source": [
        "class MovielensModel(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self, rating_weight: float, retrieval_weight: float) -> None:\n",
        "    # We take the loss weights in the constructor: this allows us to instantiate\n",
        "    # several model objects with different loss weights.\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    embedding_dimension = 32\n",
        "\n",
        "    # User and movie models.\n",
        "    self.movie_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_movie_titles, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
        "    ])\n",
        "    self.user_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_user_ids, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # A small model to take in user and movie embeddings and predict ratings.\n",
        "    # We can make this as complicated as we want as long as we output a scalar\n",
        "    # as our prediction.\n",
        "    self.rating_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ])\n",
        "\n",
        "    # The tasks.\n",
        "    self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
        "    )\n",
        "    self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
        "        metrics=tfrs.metrics.FactorizedTopK(\n",
        "            candidates=movies.batch(128).map(self.movie_model)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # The loss weights.\n",
        "    self.rating_weight = rating_weight\n",
        "    self.retrieval_weight = retrieval_weight\n",
        "\n",
        "  def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
        "    # We pick out the user features and pass them into the user model.\n",
        "    user_embeddings = self.user_model(features[\"user_id\"])\n",
        "    # And pick out the movie features and pass them into the movie model.\n",
        "    movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
        "\n",
        "    return (\n",
        "        user_embeddings,\n",
        "        movie_embeddings,\n",
        "        # We apply the multi-layered rating model to a concatentation of\n",
        "        # user and movie embeddings.\n",
        "        self.rating_model(\n",
        "            tf.concat([user_embeddings, movie_embeddings], axis=1)\n",
        "        ),\n",
        "    )\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "\n",
        "    ratings = features.pop(\"user_rating\")\n",
        "\n",
        "    user_embeddings, movie_embeddings, rating_predictions = self(features)\n",
        "\n",
        "    # We compute the loss for each task.\n",
        "    rating_loss = self.rating_task(\n",
        "        labels=ratings,\n",
        "        predictions=rating_predictions,\n",
        "    )\n",
        "    retrieval_loss = self.retrieval_task(user_embeddings, movie_embeddings)\n",
        "\n",
        "    # And combine them using the loss weights.\n",
        "    return (self.rating_weight * rating_loss\n",
        "            + self.retrieval_weight * retrieval_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngvn-c0b8lc2"
      },
      "source": [
        "### Rating-specialized model\n",
        "\n",
        "Depending on the weights we assign, the model will encode a different balance of the tasks. Let's start with a model that only considers ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNfB6rYL0VrS"
      },
      "outputs": [],
      "source": [
        "model = MovielensModel(rating_weight=1.0, retrieval_weight=0.0)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6kjfF1j0iZR"
      },
      "outputs": [],
      "source": [
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NWadH1q0c_T"
      },
      "outputs": [],
      "source": [
        "model.fit(cached_train, epochs=3)\n",
        "metrics = model.evaluate(cached_test, return_dict=True)\n",
        "\n",
        "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
        "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lENViv04-i0T"
      },
      "source": [
        "The model does OK on predicting ratings (with an RMSE of around 1.11), but performs poorly at predicting which movies will be watched or not: its accuracy at 100 is almost 4 times worse than a model trained solely to predict watches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPYd9LtE-4Fm"
      },
      "source": [
        "### Retrieval-specialized model\n",
        "\n",
        "Let's now try a model that focuses on retrieval only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfnkGd2G--Qt"
      },
      "outputs": [],
      "source": [
        "model = MovielensModel(rating_weight=0.0, retrieval_weight=1.0)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCCBdM7U_B11"
      },
      "outputs": [],
      "source": [
        "model.fit(cached_train, epochs=3)\n",
        "metrics = model.evaluate(cached_test, return_dict=True)\n",
        "\n",
        "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
        "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjM7j7eY_jPh"
      },
      "source": [
        "We get the opposite result: a model that does well on retrieval, but poorly on predicting ratings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOFwjUus_pLU"
      },
      "source": [
        "### Joint model\n",
        "\n",
        "Let's now train a model that assigns positive weights to both tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xyDbNMf_t8a"
      },
      "outputs": [],
      "source": [
        "model = MovielensModel(rating_weight=1.0, retrieval_weight=1.0)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pZmM_ub_uEO"
      },
      "outputs": [],
      "source": [
        "model.fit(cached_train, epochs=3)\n",
        "metrics = model.evaluate(cached_test, return_dict=True)\n",
        "\n",
        "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
        "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni_rkOsaB3f9"
      },
      "source": [
        "The result is a model that performs roughly as well on both tasks as each specialized model.\n",
        "\n",
        "### Making prediction\n",
        "\n",
        "We can use the trained multitask model to get trained user and movie embeddings, as well as the predicted rating:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXXh-PLaH_Vn"
      },
      "outputs": [],
      "source": [
        "trained_movie_embeddings, trained_user_embeddings, predicted_rating = model({\n",
        "      \"user_id\": np.array([\"42\"]),\n",
        "      \"movie_title\": np.array([\"Dances with Wolves (1990)\"])\n",
        "  })\n",
        "print(\"Predicted rating:\")\n",
        "print(predicted_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FADp0pUWINTD"
      },
      "source": [
        "While the results here do not show a clear accuracy benefit from a joint model in this case, multi-task learning is in general an extremely useful tool. We can expect better results when we can transfer knowledge from a data-abundant task (such as clicks) to a closely related data-sparse task (such as purchases)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7xYDRHQIQvm"
      },
      "outputs": [],
      "source": [
        "# 1. Extract Movie Embeddings\n",
        "movie_embeddings = model.movie_model.predict(unique_movie_titles)\n",
        "\n",
        "# 2. Build the ScaNN Index\n",
        "# Configure ScaNN searcher\n",
        "searcher = scann.scann_ops_pybind.builder(movie_embeddings, 50, \"dot_product\").tree(\n",
        "    num_leaves=2000, num_leaves_to_search=100, training_sample_size=250000).score_ah(\n",
        "    2, anisotropic_quantization_threshold=0.2).reorder(100).build()\n",
        "\n",
        "# 3. Get User Embedding\n",
        "user_id = \"42\"  # Replace with the actual user ID\n",
        "user_embedding = model.user_model.predict(np.array([user_id]))\n",
        "\n",
        "# 4. Query ScaNN for Nearest Neighbors\n",
        "neighbors, distances = searcher.search(user_embedding, final_num_neighbors=100)\n",
        "\n",
        "# 5. Retrieve Movie Titles\n",
        "top_k_movie_indices = neighbors\n",
        "top_k_movie_titles = unique_movie_titles[top_k_movie_indices]\n",
        "\n",
        "# 6. Predict Ratings for Top-k Movies\n",
        "user_id_array = np.array([user_id] * len(top_k_movie_titles))\n",
        "_, _, predicted_ratings = model({\n",
        "    \"user_id\": user_id_array,\n",
        "    \"movie_title\": top_k_movie_titles\n",
        "})\n",
        "\n",
        "# 7. Sort and Select Top-50\n",
        "# Combine movie titles and predicted ratings\n",
        "movie_ratings = list(zip(top_k_movie_titles, predicted_ratings.numpy().flatten()))\n",
        "\n",
        "# Sort by predicted rating (descending)\n",
        "movie_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Select top-50\n",
        "top_50_recommendations = movie_ratings[:50]\n",
        "\n",
        "# Print the recommendations\n",
        "print(f\"Top 50 recommendations for user {user_id}:\")\n",
        "for title, rating in top_50_recommendations:\n",
        "    print(f\"{title}: {rating:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
